model:
  # model parallel training が可能なモデルについては、device_map: auto とすることで自動的にモデルを分割して複数GPUに読み込んでくれる
  device_map: auto
  # 8bit/4bit training may be incompatible with V100.
  # load_in_8bit: true
  trust_remote_code: true
  torch_dtype: torch.float16

lora:
  r: 4
  lora_alpha: 4
  target_modules:
    # for GPT-NEOX (includes databricks/dolly-v2-xx, cyberagent/open-calm-xx)
    # - query_key_value
    # - dense
    # - dense_h_to_4h
    # - dense_4h_to_h
    # for GPT-2
    - c_attn
    - c_proj
    - c_fc
  lora_dropout: 0.01
  bias: none
  task_type: CAUSAL_LM
  # for GPT-2
  fan_in_fan_out: true


training:
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 1
  num_train_epochs: 1
  fp16: false
  optim: "adamw_torch"
  learning_rate: 1.0e-4
  logging_steps: 10
  evaluation_strategy: "steps"
  save_strategy: "steps"
  eval_steps: 50
  save_steps: 50
  save_total_limit: 3

# early stopping をしない場合はコメントアウトする
early_stopping_callback:
  early_stopping_patience: 5

dataset:
  path: kunishou/databricks-dolly-15k-ja

prompt_template: |-
  ### Instruction:
  {instruction}
  ### Input:
  {input}
  ### Response:
  {output}

outputs:
  # GPT-2の場合は PEFTモデルのマージがさぽーとされていないので false にする必要がある
  save_merged_model: false
  dirname: /scratch/${USER}/models/${JOB_ID}
